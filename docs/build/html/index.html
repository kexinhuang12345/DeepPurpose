
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>DeepPurpose documentation! &#8212; DeepPurpose 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <img alt="_images/logo_deeppurpose_horizontal.png" src="_images/logo_deeppurpose_horizontal.png" />
<div class="section" id="deeppurpose-documentation">
<h1>DeepPurpose documentation!<a class="headerlink" href="#deeppurpose-documentation" title="Permalink to this headline">¶</a></h1>
<p>Welcome! This is the documentation for DeepPurpose, a PyTorch-based deep learning library for Drug Target Interaction.
The Github repository is located <a class="reference external" href="https://github.com/kexinhuang12345/DeepPurpose">here</a>.</p>
<div class="section" id="how-to-start">
<h2>1 How to Start<a class="headerlink" href="#how-to-start" title="Permalink to this headline">¶</a></h2>
<div class="section" id="download">
<h3>1.1 Download<a class="headerlink" href="#download" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ git clone git@github.com:kexinhuang12345/DeepPurpose.git
$ <span class="c1">###  Download code repository</span>
$
$
$ <span class="nb">cd</span> DeepPurpose
$ <span class="c1">### Change directory to DeepPurpose</span>
</pre></div>
</div>
</div>
<div class="section" id="installation">
<h3>1.2 Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda env create -f env.yml
$ <span class="c1">## Build virtual environment with all packages installed using conda</span>
$
$ conda activate DeepPurpose
$ <span class="c1">##  Activate conda environment</span>
$
$
$ conda deactivate <span class="c1">### exit</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="run">
<h2>2 Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="documentation">
<h2>3 Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="encoder-models">
<h3>3.1 Encoder Models<a class="headerlink" href="#encoder-models" title="Permalink to this headline">¶</a></h3>
<p><strong>environment</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">SequentialSampler</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference external" href="https://arxiv.org/pdf/1908.06760.pdf">Transformer</a> can be used to encode both drug and protein on <a class="reference external" href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system">SMILES</a>.</p>
<p>name of function: <strong>constructor</strong> create Transformer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoding</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>encoding</strong> (string, “drug” or “protein”) - specify input type of the model, “drug” or “protein”.</p></li>
<li><dl class="simple">
<dt><strong>config</strong> (kwargs, keyword arguments) - specify the parameter of transformer. The keys include</dt><dd><ul>
<li><p>transformer_dropout_rate (float) - dropout rate of transformer.</p></li>
<li><p>input_dim_drug (int) - input dimension when encoding drug.</p></li>
<li><p>transformer_emb_size_drug (int) - dimension of embedding in input layer when encoding drug.</p></li>
<li><p>transformer_n_layer_drug (int) - number of layers in transformer when encoding drug.</p></li>
<li><p><strong>todo</strong></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of MPNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>v</strong> (tuple of length 2) - input feature of transformer. v[0] (np.array) is index of atoms. v[1] (np.array) is the corresponding mask.</p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN (Convolutional Neural Network)</a> can be used to encode both drug and protein on <a class="reference external" href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system">SMILES</a>.</p>
<p><strong>constructor</strong> create CNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoding</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>encoding</strong> (string, “drug” or “protein”) - specify input type of model, “drug” or “protein”.</p></li>
<li><dl class="simple">
<dt><strong>config</strong> (kwargs, keyword arguments) - specify the parameter of CNN. The keys include</dt><dd><ul>
<li><p>cnn_drug_filters (list, each element is int) - specify the size of filter when encoding drug, e.g., cnn_drug_filters = [32,64,96].</p></li>
<li><p>cnn_drug_kernels (list, each element is int) - specify the size of kernel when encoding drug, e.g., cnn_drug_kernels = [4,6,8].</p></li>
<li><p>hidden_dim_drug (int) - specify the hidden dimension when encoding drug, e.g., hidden_dim_drug = 256.</p></li>
<li><p>cnn_target_filters (list, each element is int) - specify the size of filter when encoding protein, e.g, cnn_target_filters = [32,64,96].</p></li>
<li><p>cnn_target_kernels (list, each element is int) - specify the size of kernel when encoding protein, e.g, cnn_target_kernels = [4,8,12].</p></li>
<li><p>hidden_dim_protein (int) - specify the hidden dimension when encoding protein, e.g., hidden_dim_protein = 256.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of CNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>v</strong> (torch.Tensor) - input feature of CNN.</p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">CNN_RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p>CNN_RNN means a GRU/LSTM on top of a CNN on <a class="reference external" href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system">SMILES</a>.</p>
<p><strong>constructor</strong> create CNN_RNN</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoding</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>encoding</strong> (string, “drug” or “protein”) - specify input type, “drug” or “protein”.</p></li>
<li><dl class="simple">
<dt><strong>config</strong> (kwargs, keyword arguments) - specify the parameter of transformer. The keys include</dt><dd><ul>
<li><p>cnn_drug_filters (list, each element is int) - specify the size of filter when encoding drug, e.g., cnn_drug_filters = [32,64,96].</p></li>
<li><p>cnn_drug_kernels (list, each element is int) - specify the size of kernel when encoding drug, e.g., cnn_drug_kernels = [4,6,8].</p></li>
<li><p>rnn_drug_hid_dim (int) - specify the hidden dimension of RNN when encoding drug, e.g., rnn_drug_hid_dim = 64.</p></li>
<li><p>rnn_drug_n_layers (int) - specify number of layer in RNN when encoding drug, .e.g, rnn_drug_n_layers = 2.</p></li>
<li><p>rnn_drug_bidirectional (bool) - specify if RNN is bidirectional when encoding drug, .e.g, rnn_drug_bidirectional = True.</p></li>
<li><p>hidden_dim_drug (int) - specify the hidden dimension when encoding drug, e.g., hidden_dim_drug = 256.</p></li>
<li><p>cnn_target_filters (list, each element is int) - specify the size of filter when encoding protein, e.g, cnn_target_filters = [32,64,96].</p></li>
<li><p>cnn_target_kernels (list, each element is int) - specify the size of kernel when encoding protein, e.g, cnn_target_kernels = [4,8,12].</p></li>
<li><p>hidden_dim_protein (int) - specify the hidden dimension when encoding protein, e.g., hidden_dim_protein = 256.</p></li>
<li><p>rnn_target_hid_dim (int) - specify hidden dimension of RNN when encoding protein, e.g., rnn_target_hid_dim = 64.</p></li>
<li><p>rnn_target_n_layers (int) - specify the number of layer in RNN when encoding protein, e.g., rnn_target_n_layers = 2.</p></li>
<li><p>rnn_target_bidirectional (bool) - specify if RNN is bidirectional when encoding protein, e.g., rnn_target_bidirectional = True</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of CNN_RNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>v</strong> (torch.Tensor) - input feature of CNN_RNN.</p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p>Multi-Layer Perceptron</p>
<p><strong>constructor</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>input_dim</strong> (int) - dimension of input feature.</p></li>
<li><p><strong>hidden_dim</strong> (int) - dimension of hidden layer.</p></li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of MLP.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>v</strong></p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MPNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference external" href="https://www.biorxiv.org/content/10.1101/684662v3">Message Passing Neural Network (MPNN)</a> encode drug in its graph representation.</p>
<p><strong>constructor</strong> create MPNN class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mpnn_hidden_size</span><span class="p">,</span> <span class="n">mpnn_depth</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>mpnn_hidden_size</strong> (int) - specify dimension of hidden layer in MPNN, e.g,  mpnn_hidden_size = 256.</p></li>
<li><p><strong>mpnn_depth</strong> (int) - specify depth of MPNN, e.g.,  mpnn_depth = 3.</p></li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of MPNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><dl class="simple">
<dt><strong>feature</strong> (tuple of length 5)</dt><dd><ul>
<li><p><strong>todo</strong></p></li>
<li></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="classifier">
<h3>3.2 Classifier<a class="headerlink" href="#classifier" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
</pre></div>
</div>
<p>Classifier is make the prediction for DBTA, it serve as a basic component of class DBTA.</p>
<p><strong>constructor</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_drug</span><span class="p">,</span> <span class="n">model_protein</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>model_drug</strong> (DeepPurpose.models.XX) - Encoder model for drug. XX can be “transformer”, “MPNN”, “CNN”, “CNN_RNN” …,</p></li>
<li><p><strong>model_protein</strong> (DeepPurpose.models.XX) - Encoder model for protein. XX can be “transformer”, “CNN”, “CNN_RNN” …,</p></li>
<li><p><strong>config</strong> (kwargs, keyword arguments) - specify the parameter of classifier.</p></li>
</ul>
<p><strong>Calling functions</strong> implement the feedforward procedure of Classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_D</span><span class="p">,</span> <span class="n">v_P</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>v_D</strong> (many types) - input feature for drug encoder model, like “DeepPurpose.models.transformer”, “DeepPurpose.models.CNN”, “DeepPurpose.models.CNN_RNN”, “DeepPurpose.models.MPNN”.</p></li>
<li><p><strong>v_P</strong> (many types) - input feature for protein encoder model, like “DeepPurpose.models.transformer”, “DeepPurpose.models.CNN”, “DeepPurpose.models.CNN_RNN”.</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="drug-target-binding-affinity">
<h3>3.3 Drug Target Binding Affinity<a class="headerlink" href="#drug-target-binding-affinity" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepPurpose</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">DBTA</span>
</pre></div>
</div>
<p>Drug Target Binding Affinity (DBTA) model include all learning procedure.</p>
<p>name of function: <strong>constructor</strong> create DBTA model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><dl class="simple">
<dt><strong>config</strong> (kwargs, keyword arguments) - specify the parameter of DBTA.</dt><dd><ul>
<li><p><strong>drug_encoding</strong> (str) - Encoder mode for drug. It can be “transformer”, “MPNN”, “CNN”, “CNN_RNN” …,</p></li>
<li><p><strong>target_encoding</strong> (str) - Encoder mode for protein. It can be “transformer”, “CNN”, “CNN_RNN” …,</p></li>
<li><p><strong>result_folder</strong> (str) - directory that store the learning log/results.</p></li>
<li><p><strong>concrete parameter for encoder model</strong> (repeated)</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>name of function: <strong>test_</strong> include all the inference procedure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_generator</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">repurposing_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>data_generator</strong> (iterator) - iterator of torch.utils.data.DataLoader. It can be test data or validation data.</p></li>
<li><p><strong>model</strong> (DeepPurpose.models.Classifier) - model of DBTA.</p></li>
<li><p><strong>repurposing_mode</strong> (bool) - If repurposing_mode is True, then do repurposing. Otherwise, do compute the accuracy (including AUC score).</p></li>
<li><p><strong>test</strong> (bool) - If test is True, plot ROC-AUC and PR-AUC curve. Otherwise, pass.</p></li>
</ul>
<p>name of function: <strong>train</strong> include all the training procedure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>train</strong> () - Train data generator</p></li>
<li><p><strong>val</strong> () - Valid data generator</p></li>
<li><p><strong>test</strong> () - Test data generator</p></li>
<li><p><strong>verbose</strong> (bool) - If verbose is True, then print training record every 100 iterations.</p></li>
</ul>
<p>name of function: <strong>predict</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df_data</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>df_data</strong> (pd.DataFrame) - specify data that we need to predict.</p></li>
</ul>
<p>name of function: <strong>save_model</strong> save the well-trained model to specific directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path_dir</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>path_dir</strong> (str, a directory) - the path where model is saved.</p></li>
</ul>
<p>name of function: <strong>load_pretrained</strong> load the well-trained model so that we are able to make inference directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">load_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>path</strong> (str, a directory) - the path where model is loaded.</p></li>
</ul>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">DeepPurpose</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Kexin Huang, Tianfan Fu.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>