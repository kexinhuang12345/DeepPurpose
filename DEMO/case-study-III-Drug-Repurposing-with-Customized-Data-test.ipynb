{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:33:07] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "Loading customized repurposing dataset...\n",
      "Beginning Downloading Configs Files for training from scratch...\n",
      "Downloading finished... Beginning to extract zip file...\n",
      "Configs Models Successfully Downloaded...\n",
      "Training on your own customized data...\n",
      "in total: 26640 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 13763\n",
      "drug encoding finished...\n",
      "encoding protein...\n",
      "unique target sequence: 1\n",
      "protein encoding finished...\n",
      "splitting dataset...\n",
      "Done.\n",
      "Training from scrtach...\n",
      "Begin to train model 0 with drug encoding MPNN and target encoding CNN\n",
      "Let's use 1 GPU!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 0.69326. Total time 0.00055 hours\n",
      "Training at Epoch 1 iteration 100 with loss 0.70115. Total time 0.01472 hours\n",
      "Training at Epoch 1 iteration 200 with loss 0.69409. Total time 0.03 hours\n",
      "Training at Epoch 1 iteration 300 with loss 0.69345. Total time 0.045 hours\n",
      "Training at Epoch 1 iteration 400 with loss 0.69598. Total time 0.05833 hours\n",
      "Training at Epoch 1 iteration 500 with loss 0.69608. Total time 0.06833 hours\n",
      "Training at Epoch 1 iteration 600 with loss 0.68977. Total time 0.08388 hours\n",
      "Validation at Epoch 1 , AUROC: 0.58969 , AUPRC: 0.04194 , F1: 0.06469\n",
      "Training at Epoch 2 iteration 0 with loss 0.69977. Total time 0.09972 hours\n",
      "Training at Epoch 2 iteration 100 with loss 0.69107. Total time 0.11527 hours\n",
      "Training at Epoch 2 iteration 200 with loss 0.69532. Total time 0.12555 hours\n",
      "Training at Epoch 2 iteration 300 with loss 0.69316. Total time 0.13861 hours\n",
      "Training at Epoch 2 iteration 400 with loss 0.69270. Total time 0.15444 hours\n",
      "Training at Epoch 2 iteration 500 with loss 0.69233. Total time 0.17027 hours\n",
      "Training at Epoch 2 iteration 600 with loss 0.70020. Total time 0.18361 hours\n",
      "Validation at Epoch 2 , AUROC: 0.5 , AUPRC: 0.03343 , F1: 0.06469\n",
      "Training at Epoch 3 iteration 0 with loss 0.70158. Total time 0.19472 hours\n",
      "Training at Epoch 3 iteration 100 with loss 0.69314. Total time 0.20972 hours\n",
      "Training at Epoch 3 iteration 200 with loss 0.68508. Total time 0.22583 hours\n",
      "Training at Epoch 3 iteration 300 with loss 0.69328. Total time 0.24138 hours\n",
      "Training at Epoch 3 iteration 400 with loss 0.69174. Total time 0.25138 hours\n",
      "Training at Epoch 3 iteration 500 with loss 0.69125. Total time 0.26416 hours\n",
      "Training at Epoch 3 iteration 600 with loss 0.69791. Total time 0.27944 hours\n",
      "Validation at Epoch 3 , AUROC: 0.5 , AUPRC: 0.03343 , F1: 0.06469\n",
      "Training at Epoch 4 iteration 0 with loss 0.69316. Total time 0.29527 hours\n",
      "Training at Epoch 4 iteration 100 with loss 0.68676. Total time 0.30833 hours\n",
      "Training at Epoch 4 iteration 200 with loss 0.69372. Total time 0.31805 hours\n",
      "Training at Epoch 4 iteration 300 with loss 0.69290. Total time 0.33305 hours\n",
      "Training at Epoch 4 iteration 400 with loss 0.69419. Total time 0.34861 hours\n",
      "Training at Epoch 4 iteration 500 with loss 0.69087. Total time 0.36416 hours\n",
      "Training at Epoch 4 iteration 600 with loss 0.69333. Total time 0.375 hours\n",
      "Validation at Epoch 4 , AUROC: 0.5 , AUPRC: 0.03343 , F1: 0.0\n",
      "Training at Epoch 5 iteration 0 with loss 0.69256. Total time 0.3875 hours\n",
      "Training at Epoch 5 iteration 100 with loss 0.69360. Total time 0.40277 hours\n",
      "Training at Epoch 5 iteration 200 with loss 0.69720. Total time 0.41805 hours\n",
      "Training at Epoch 5 iteration 300 with loss 0.69478. Total time 0.43222 hours\n",
      "Training at Epoch 5 iteration 400 with loss 0.69394. Total time 0.44138 hours\n",
      "Training at Epoch 5 iteration 500 with loss 0.69489. Total time 0.45666 hours\n",
      "Training at Epoch 5 iteration 600 with loss 0.69347. Total time 0.47222 hours\n",
      "Validation at Epoch 5 , AUROC: 0.5 , AUPRC: 0.03343 , F1: 0.0\n",
      "Training at Epoch 6 iteration 0 with loss 0.69603. Total time 0.48833 hours\n",
      "Training at Epoch 6 iteration 100 with loss 0.67906. Total time 0.49944 hours\n",
      "Training at Epoch 6 iteration 200 with loss 0.69459. Total time 0.51166 hours\n",
      "Training at Epoch 6 iteration 300 with loss 0.69419. Total time 0.52694 hours\n",
      "Training at Epoch 6 iteration 400 with loss 0.69154. Total time 0.5425 hours\n",
      "Training at Epoch 6 iteration 500 with loss 0.68590. Total time 0.55666 hours\n",
      "Training at Epoch 6 iteration 600 with loss 0.69321. Total time 0.56583 hours\n",
      "Validation at Epoch 6 , AUROC: 0.5 , AUPRC: 0.03343 , F1: 0.0\n",
      "Training at Epoch 7 iteration 0 with loss 0.69307. Total time 0.58138 hours\n",
      "Training at Epoch 7 iteration 100 with loss 0.69406. Total time 0.59638 hours\n",
      "Training at Epoch 7 iteration 200 with loss 0.69256. Total time 0.61166 hours\n",
      "Training at Epoch 7 iteration 300 with loss 0.69468. Total time 0.62333 hours\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d8e37a6fa51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m oneliner.repurpose(*load_SARS_CoV_Protease_3CL(), *load_antiviral_drugs(no_cid = True),  *read_file_training_dataset_bioassay(FILE_PATH), \\\n\u001b[0;32m----> 7\u001b[0;31m     split='HTS', convert_y = False, frac=[0.8,0.1,0.1], pretrained = False, agg = 'max_effect')\n\u001b[0m",
      "\u001b[0;32m~/DeepPurpose/oneliner.py\u001b[0m in \u001b[0;36mrepurpose\u001b[0;34m(target, target_name, X_repurpose, drug_names, train_drug, train_target, train_y, save_dir, pretrained_dir, finetune_epochs, finetune_LR, finetune_batch_size, convert_y, subsample_frac, pretrained, split, frac, agg, output_len)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_folder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_folder_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model training finished, now repurposing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepPurpose/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, val, test, verbose)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DeepPurpose/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DeepPurpose/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DeepPurpose import oneliner\n",
    "from DeepPurpose.dataset import *\n",
    "\n",
    "FILE_PATH = load_AID1706_txt_file()\n",
    "\n",
    "oneliner.repurpose(*load_SARS_CoV_Protease_3CL(), *load_antiviral_drugs(no_cid = True),  *read_file_training_dataset_bioassay(FILE_PATH), \\\n",
    "    split='HTS', convert_y = False, frac=[0.8,0.1,0.1], pretrained = False, agg = 'agg_mean_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
